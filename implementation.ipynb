{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define global params for training\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 50\n",
    "EMBEDDING_SIZE = 50\n",
    "DEBUG_LENGTH = 10\n",
    "HIDDEN_SIZE = 100\n",
    "UNIQUE_WORDS = 2000\n",
    "\n",
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 1\n",
    "EOS_token = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points, points2):\n",
    "#     plt.figure()\n",
    "#     fig, ax = plt.subplots()\n",
    "#     # this locator puts ticks at regular intervals\n",
    "#     loc = ticker.MultipleLocator(base=1)\n",
    "#     ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)\n",
    "    plt.plot(points2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(language):\n",
    "    with open('data/train.{}.txt'.format(language), 'r') as data:\n",
    "        sentences = [sentence.rstrip('\\n').split(' ') for sentence in data]\n",
    "    with open('data/vocab.{}.txt'.format(language), 'r') as vocab:\n",
    "        vocab = {word.rstrip('\\n'): index for index, word in enumerate(vocab)}\n",
    "    return sentences, vocab\n",
    "\n",
    "en_sentences, en_vocab = load_dataset('en')\n",
    "vi_sentences, vi_vocab = load_dataset('vi')\n",
    "\n",
    "dataset = list(zip(en_sentences, vi_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(sentences):\n",
    "    language_dict = dict()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            if word in language_dict:\n",
    "                language_dict[word] += 1\n",
    "            else:\n",
    "                language_dict[word] = 1\n",
    "    word_frequency = [(word, language_dict[word]) for word in language_dict]\n",
    "    sorted_word_frequency = sorted(word_frequency, key=lambda x: x[1], reverse=True)\n",
    "    if len(sorted_word_frequency) > UNIQUE_WORDS - 3:\n",
    "        sorted_word_frequency = sorted_word_frequency[:UNIQUE_WORDS - 3]\n",
    "    new_dict = {'<unk>': 0, '<s>': 1, '</s>': 2}\n",
    "    counter = 3\n",
    "    for word_tuple in sorted_word_frequency:\n",
    "        if word_tuple[0] not in new_dict:\n",
    "            new_dict[word_tuple[0]] = counter\n",
    "            counter += 1\n",
    "    return new_dict\n",
    "\n",
    "en_vocab = create_vocab(en_sentences)\n",
    "vi_vocab = create_vocab(vi_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_sets(in_language, out_language):\n",
    "    with open('data/test.{}.txt'.format(in_language), 'r') as data:\n",
    "        in_sentences = [sentence.rstrip('\\n').split(' ') for sentence in data]\n",
    "    with open('data/test.{}.txt'.format(out_language), 'r') as data:\n",
    "        out_sentences = [sentence.rstrip('\\n').split(' ') for sentence in data]\n",
    "    return in_sentences, out_sentences\n",
    "\n",
    "en_test_sentences, vi_test_sentences = load_test_sets('en', 'vi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_from_dict(word, lan_dict):\n",
    "    if word in lan_dict:\n",
    "        return lan_dict[word]\n",
    "    else:\n",
    "        return lan_dict['<unk>']\n",
    "\n",
    "def process_sentences(sentences, vocab, translate_to=False):\n",
    "    X = list()\n",
    "    Xoh = list()\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if translate_to:\n",
    "            index_sentence = [SOS_token] + [word_from_dict(word, vocab) for word in sentence] + [EOS_token]\n",
    "        else:\n",
    "            index_sentence = [word_from_dict(word, vocab) for word in sentence] + [EOS_token]\n",
    "        a = np.array(index_sentence)\n",
    "        b = np.zeros((a.size, len(vocab)))\n",
    "        b[np.arange(a.size),a] = 1\n",
    "        X.append(index_sentence)\n",
    "        Xoh.append(b)\n",
    "    X = np.array([np.array(Xi) for Xi in X])\n",
    "    Xoh = np.array([Xohi for Xohi in Xoh])\n",
    "    return X, Xoh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Xoh = process_sentences(en_sentences, en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y, Yoh = process_sentences(vi_sentences, vi_vocab, translate_to=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Xoh_test = process_sentences(en_test_sentences, en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test, Yoh_test = process_sentences(vi_test_sentences, vi_vocab, translate_to=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, EMBEDDING_SIZE)\n",
    "        self.gru = nn.GRU(EMBEDDING_SIZE, hidden_size, bidirectional=True, batch_first=True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(2, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 3, 1)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_output):\n",
    "        decoder_hidden_expanded = decoder_hidden.expand(1, encoder_output.size()[1], decoder_hidden.size()[2])\n",
    "        input_vector = torch.cat((decoder_hidden_expanded, encoder_output), 2)\n",
    "        output = torch.matmul(input_vector, self.attn.weight.t())\n",
    "        attn_weights = F.softmax(output, dim=1)\n",
    "        permuted_encoder_output = encoder_output.permute(0, 2, 1)\n",
    "        input_context = torch.bmm(permuted_encoder_output, attn_weights).view(1,1,-1)\n",
    "        return input_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, EMBEDDING_SIZE)\n",
    "        self.gru = nn.GRU(self.hidden_size * 2 + EMBEDDING_SIZE, self.hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(self.hidden_size, input_size)\n",
    "\n",
    "    def forward(self, input_context, hidden, word):\n",
    "        embedded = self.embedding(word)\n",
    "        output = torch.cat((embedded, input_context), 2)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(in_sentence, out_sentence, model_sections, optimizing_params):\n",
    "    # pass input sentence through encoder\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    input = Variable(torch.LongTensor(in_sentence).view(1,-1))\n",
    "    input = input.cuda() if use_cuda else input\n",
    "    encoder_output, encoder_hidden = model_sections['encoder'](input, encoder_hidden)\n",
    "    \n",
    "    # initialize decoder hidden layer with final encoder hidden layer\n",
    "    decoder_hidden = encoder_hidden[0].clone().view(1,1,-1)\n",
    "    \n",
    "    # initialize loss to 0\n",
    "    loss = 0 \n",
    "    \n",
    "    # pass encoder output through attention + decoder\n",
    "    for word_index in out_sentence:\n",
    "        decoder_input = Variable(torch.LongTensor([[int(word_index)]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        input_context = model_sections['attention'](decoder_hidden, encoder_output) \n",
    "        decoder_output, decoder_hidden = model_sections['decoder'](input_context, decoder_hidden, decoder_input)\n",
    "        loss += optimizing_params['loss_function'](decoder_output.view(1,-1), decoder_input.view(-1))\n",
    "    loss.backward()\n",
    "    optimizing_params['enc_optimizer'].step()\n",
    "    optimizing_params['att_optimizer'].step()\n",
    "    optimizing_params['dec_optimizer'].step()\n",
    "    \n",
    "    return loss.data[0] / len(out_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(in_sentence, out_sentence, model_sections, optimizing_params):\n",
    "    # pass input sentence through encoder\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "    input = Variable(torch.LongTensor(in_sentence).view(1,-1))\n",
    "    input = input.cuda() if use_cuda else input\n",
    "    encoder_output, encoder_hidden = model_sections['encoder'](input, encoder_hidden)\n",
    "    \n",
    "    # initialize decoder hidden layer with final encoder hidden layer\n",
    "    decoder_hidden = encoder_hidden[0].clone().view(1,1,-1)\n",
    "    \n",
    "    # initialize loss to 0\n",
    "    loss = 0 \n",
    "    \n",
    "    # pass encoder output through attention + decoder\n",
    "    for word_index in out_sentence:\n",
    "        decoder_input = Variable(torch.LongTensor([[int(word_index)]]))\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        input_context = model_sections['attention'](decoder_hidden, encoder_output) \n",
    "        decoder_output, decoder_hidden = model_sections['decoder'](input_context, decoder_hidden, decoder_input)\n",
    "        loss += optimizing_params['loss_function'](decoder_output.view(1,-1), decoder_input.view(-1))\n",
    "    return loss.data[0] / len(out_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(in_sentences, out_sentences, in_test_sentences, out_test_sentences, model_sections, optimizing_params):\n",
    "    training_losses = list()\n",
    "    testing_losses = list()\n",
    "    current_loss = 0\n",
    "    testing_loss = 0\n",
    "    for epoch_num in range(EPOCHS):\n",
    "        if DEBUG:\n",
    "            train_set_length = DEBUG_LENGTH\n",
    "            test_set_length = DEBUG_LENGTH\n",
    "        else:\n",
    "            train_set_length = len(in_sentences)\n",
    "            test_set_length = len(in_test_sentences)\n",
    "        for in_sentence, out_sentence in zip(in_sentences[:train_set_length], out_sentences[:train_set_length]):\n",
    "            current_loss += train_step(in_sentence, out_sentence, model_sections, optimizing_params)\n",
    "        for in_test_sentence, out_test_sentence in zip(in_test_sentences[:test_set_length], out_test_sentences[:test_set_length]):\n",
    "            testing_loss += test_step(in_test_sentence, out_test_sentence, model_sections, optimizing_params)\n",
    "               \n",
    "        print('Epoch: {} | TRAINING Loss: {} | TESTING Loss: {}'.format(epoch_num + 1, current_loss / train_set_length, testing_loss / test_set_length))\n",
    "        training_losses.append(current_loss / train_set_length)\n",
    "        testing_losses.append(testing_loss  / test_set_length)\n",
    "        current_loss = 0\n",
    "        testing_loss = 0\n",
    "    return training_losses, testing_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model and optimizer\n",
    "encoder = EncoderRNN(len(en_vocab), HIDDEN_SIZE)\n",
    "attention = Attention(HIDDEN_SIZE)\n",
    "decoder = DecoderRNN(len(vi_vocab), HIDDEN_SIZE)\n",
    "criterion = nn.NLLLoss()\n",
    "encoder_optimizer = optim.SGD(encoder.parameters(), lr=LEARNING_RATE)\n",
    "attention_optimizer = optim.SGD(attention.parameters(), lr=LEARNING_RATE)\n",
    "decoder_optimizer = optim.SGD(decoder.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# save model and optimizer params to dict\n",
    "model_sections = {'encoder': encoder, \n",
    "                  'attention': attention, \n",
    "                  'decoder': decoder,\n",
    "                 }\n",
    "\n",
    "optimizing_params = {'loss_function': criterion, \n",
    "                     'enc_optimizer': encoder_optimizer,\n",
    "                     'att_optimizer': attention_optimizer,\n",
    "                     'dec_optimizer': decoder_optimizer,\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | TRAINING Loss: 7.041870692253542 | TESTING Loss: 6.0529797361111495\n",
      "Epoch: 2 | TRAINING Loss: 3.769784379490205 | TESTING Loss: 5.611906244803806\n",
      "Epoch: 3 | TRAINING Loss: 2.1109667264369496 | TESTING Loss: 5.562807493304687\n",
      "Epoch: 4 | TRAINING Loss: 0.9091241586921562 | TESTING Loss: 7.443461373722774\n",
      "Epoch: 5 | TRAINING Loss: 0.9923537748990379 | TESTING Loss: 10.015064811790586\n",
      "Epoch: 6 | TRAINING Loss: 3.2872420182343687 | TESTING Loss: 13.634627021564672\n",
      "Epoch: 7 | TRAINING Loss: 3.1819766628423145 | TESTING Loss: 12.718148345347123\n",
      "Epoch: 8 | TRAINING Loss: 5.025639607048335 | TESTING Loss: 19.91256579191326\n",
      "Epoch: 9 | TRAINING Loss: 13.592446564740015 | TESTING Loss: 26.67644702985819\n",
      "Epoch: 10 | TRAINING Loss: 18.285023211038602 | TESTING Loss: 35.08686170985578\n",
      "Epoch: 11 | TRAINING Loss: 19.32296846974502 | TESTING Loss: 39.76358575306772\n",
      "Epoch: 12 | TRAINING Loss: 16.727240959773233 | TESTING Loss: 44.26124536231953\n",
      "Epoch: 13 | TRAINING Loss: 14.874220794166362 | TESTING Loss: 48.00416150518286\n",
      "Epoch: 14 | TRAINING Loss: 24.214880384512973 | TESTING Loss: 56.28806338011183\n",
      "Epoch: 15 | TRAINING Loss: 25.467423513123325 | TESTING Loss: 64.53207323572782\n",
      "Epoch: 16 | TRAINING Loss: 36.83280454195463 | TESTING Loss: 74.78810433501366\n",
      "Epoch: 17 | TRAINING Loss: 44.93361253317122 | TESTING Loss: 80.23486943937337\n",
      "Epoch: 18 | TRAINING Loss: 45.022676271068796 | TESTING Loss: 86.38802732864283\n",
      "Epoch: 19 | TRAINING Loss: 49.57756339597852 | TESTING Loss: 94.53837842828523\n",
      "Epoch: 20 | TRAINING Loss: 47.84496224678372 | TESTING Loss: 103.9256508551052\n",
      "Epoch: 21 | TRAINING Loss: 54.51336454750985 | TESTING Loss: 131.1295772338584\n",
      "Epoch: 22 | TRAINING Loss: 71.62152006944665 | TESTING Loss: 153.60187532365325\n",
      "Epoch: 23 | TRAINING Loss: 71.87827781234515 | TESTING Loss: 160.48456458577766\n",
      "Epoch: 24 | TRAINING Loss: 60.625278305486155 | TESTING Loss: 160.46791934421668\n",
      "Epoch: 25 | TRAINING Loss: 50.67528884609701 | TESTING Loss: 183.11840079591883\n",
      "Epoch: 26 | TRAINING Loss: 51.58851312336169 | TESTING Loss: 197.83862850366899\n",
      "Epoch: 27 | TRAINING Loss: 51.66315612744921 | TESTING Loss: 201.82412000100786\n",
      "Epoch: 28 | TRAINING Loss: 40.63894910876729 | TESTING Loss: 214.11206250570754\n",
      "Epoch: 29 | TRAINING Loss: 32.94254237389061 | TESTING Loss: 221.75516664729753\n",
      "Epoch: 30 | TRAINING Loss: 24.475015940440972 | TESTING Loss: 235.65648934885226\n",
      "Epoch: 31 | TRAINING Loss: 26.12447961331057 | TESTING Loss: 244.42874056856576\n",
      "Epoch: 32 | TRAINING Loss: 21.51139298741473 | TESTING Loss: 253.16452161257195\n",
      "Epoch: 33 | TRAINING Loss: 28.22844936643695 | TESTING Loss: 262.5472240594037\n",
      "Epoch: 34 | TRAINING Loss: 25.92606382029366 | TESTING Loss: 276.72977799997875\n",
      "Epoch: 35 | TRAINING Loss: 27.515307093940766 | TESTING Loss: 300.14123340036593\n",
      "Epoch: 36 | TRAINING Loss: 22.47842174827382 | TESTING Loss: 308.9583746422981\n",
      "Epoch: 37 | TRAINING Loss: 20.40433813915977 | TESTING Loss: 320.9407160428877\n",
      "Epoch: 38 | TRAINING Loss: 16.044898703151162 | TESTING Loss: 334.4469349587785\n",
      "Epoch: 39 | TRAINING Loss: 9.829179405708075 | TESTING Loss: 349.78904484617294\n",
      "Epoch: 40 | TRAINING Loss: 8.846588973541381 | TESTING Loss: 361.9230601374094\n",
      "Epoch: 41 | TRAINING Loss: 7.526565522474047 | TESTING Loss: 371.7681741702369\n",
      "Epoch: 42 | TRAINING Loss: 5.414186127008857 | TESTING Loss: 391.80593825873456\n",
      "Epoch: 43 | TRAINING Loss: 9.081413547912268 | TESTING Loss: 408.7852598907602\n",
      "Epoch: 44 | TRAINING Loss: 13.452206331356871 | TESTING Loss: 428.92830886906887\n",
      "Epoch: 45 | TRAINING Loss: 9.546368982404726 | TESTING Loss: 451.7169964607384\n",
      "Epoch: 46 | TRAINING Loss: 12.487702100601917 | TESTING Loss: 465.2421279746799\n",
      "Epoch: 47 | TRAINING Loss: 12.524614531392189 | TESTING Loss: 477.91720137653937\n",
      "Epoch: 48 | TRAINING Loss: 8.496247906013455 | TESTING Loss: 497.5810169130229\n",
      "Epoch: 49 | TRAINING Loss: 6.600006337733726 | TESTING Loss: 510.89324154017356\n",
      "Epoch: 50 | TRAINING Loss: 4.7103685949850975 | TESTING Loss: 535.7188575382398\n"
     ]
    }
   ],
   "source": [
    "training_losses, testing_losses = train(X, Y, X_test, Y_test, model_sections, optimizing_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntT0ZpLk/3QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x47e727780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "showPlot(training_losses, testing_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
